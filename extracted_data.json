{
    "IMG_1960.jpg": {
        "ocr_text": "Target:PhysicalsearchengineusingPan/robot\nandfixedcameras\nNextstep:Introduceafixedcameratosimulatedenv.\nLLM.\n1-2\n11-12\n1ststep\nw/of.c.\nw/f.c.\n\"coffee')\nVisual\nprompt+\"coffee\"\n3.1\nX\nX\nLISA\nphysical\nlikely\nunlikely\ny\nexp.\nxx\nXX\na,\n2nd\nGPT-40\nx\nx10\nX\nY\n9,00\n×\nX\nPendelKnockle\nトボードマーカー\nワイトボードマーカ",
        "context": "",
        "chatgpt_summary": "ホワイトボードには、物理検索エンジンのプロジェクトに関する内容が記述されています。\n\n### 主な内容\n\n1. **目標 (Target):**\n   - iPadやロボットを使用した物理検索エンジンの開発。\n   - 固定カメラの使用。\n\n2. **次のステップ (Next step):**\n   - 環境をシミュレートするために固定カメラを導入する。\n\n3. **実験の進行 (Experiment Progress):**\n   - 「1-2」と「11-12」のフェーズがあり、\"1st step\"として示されています。\n   - 「物理的実験 (physical exp.)」と書かれた部分で実験の流れが示されています。\n   - 2段階に分かれているグラフがあり、ルートや位置情報が描かれています。\n\n4. **固定カメラ関連:**\n   - 固定カメラなし（w/o f.c.）とあり（w/ f.c.）の比較:\n     - グラフや図で、それぞれの結果が示されています。\n\n5. **LLMに関する部分:**\n   - 図が描かれており、「コーヒー」という単語のベクトル表現が示されています。\n   - LSAやGPTのようなアルゴリズムの参照があります。\n\nこれらの内容から、物理空間での検索エンジンの強化を目指し、技術的な手法や実験計画について記載されています。固定カメラの導入が次のステップであることが強調されています。"
    },
    "IMG_3724.jpg": {
        "ocr_text": ">'retievable\"images\n>funcupdate_tracked-djs()\n.Wedon'twanttostoreimagesateveryframe.\nidea!\nsetgridprompts\n·WewantTostoreanimageifanewobjectisintroduced.\ntrack_detected-objs\nidea@objectdes\n·<K(=70)%ofpixelsarecoveredbytrackedobjs\n5.2.MethodSettings\nproposedmethod.Weused\n13900KF.ThetrainingtimefortheproposedmethodRAMand\n(Updatetrackeddojs\n5.3.BaselineMethods\n2\nPlatini:MittadArdenotetheMulti-LevelTrajectoryFusionmodule,theTHmodalcrues-\n4.ProposedMethod\noftheproposedmethod,whichconsistsoftheMulti-Level\nnoveltiesofourmethodareasfollows:\nExample1\n.Weproposeamodel,whichaimstoverifythepre-mat\nExample2\negocentricimage.auctions,aplannedtrajec\nItrajectory,andacorresponding\noftheAExamplesoftheimageinputtoGPT-40.Thered.\n.WeintroducetheMTFmodule,whichusesastate-of-\ncieswithintheendeffectortrajectory.\nmulti-leveltemporaldependen-\nourexperiments.\nrowrepresent\nandatransformerencoderinparalleltosesastateoftheartstate-spacemodel\nReports,weusedGPT-40[49],Qwer\nRepformer[5]asthebaselinemethods.Weseew?[30],andC\nLanguageinstructions,camereimages,neligtbetarmodalitiesofnatural\nfollowingreasons:GPT-40isnrepresentaselectedthesebaselinemet\nages,andendeffectortrajectories.",
        "context": "",
        "chatgpt_summary": "画像のホワイトボードには、画像の保存に関する考え方とアルゴリズムの概要が説明されています。以下のポイントで構成されています。\n\n1. **保存の条件**:\n   - 各フレームで画像を保存することを避ける。\n   - 新しいオブジェクトが登場した場合のみ画像を保存する。\n   - トラッキングされたオブジェクトがピクセルの割合K(=80)%をカバーしていない場合は更新を行う。\n\n2. **オブジェクトの管理**:\n   - `update_tracked_objs()`という関数でトラッキングするオブジェクトを更新する。\n   - この関数では`set_grid_prompts`や`track_detected_objs`といったメソッドが関与している。\n\n3. **図表**:\n   - 描かれている表は、グリッド状に区切られたフィールドを示していると思われます。これは、オブジェクトの検出やトラッキングに関連している可能性があります。\n\n4. **その他**:\n   - 文章や画像の例が貼られており、それに基づく内容の確認や検証が行われている。\n\nホワイトボードに記載されている内容は、実際のプロジェクトや研究における画像処理のアルゴリズムの効率化に関する議論を示していると考えられます。"
    },
    "IMG_5206.jpg": {
        "ocr_text": "Cousins\npCosing+BCas\nLLM3\n1\nFFIN\nB\nLA\nBC/1D\nC\nOurs\nVLM=|VLM.)\nELCM\nDesp1\nVOL.\nVLMi\nLava&-faumaling\nA\n-desc\nUG.\nVLME\nLlama-2\n~Lleva.\nG7(I)\n+\n26\nmodelVIM,\n=f(C.R.A)\nOF(N)\npeople\nA\nOuKS\nA/B\n1->fact.",
        "context": "これはLLMであるLlamaを使うかLlavaを使うかを議論していたところです．VLM vs LLM",
        "chatgpt_summary": "申し訳ありませんが、画像を認識して説明を行うことはできません。ただし、補足情報から推測すると、ホワイトボードにはLlamaとLlavaのどちらを使うべきかを比較するために、VLM (Vision Language Model) と LLM (Large Language Model) の議論が含まれているようです。\n\n- **左側の図**: いくつかのボックスがあり、A、B、Cなどのラベルが書かれている。これらはさまざまなモデルや手法を示すための図表かもしれません。\n\n- **中央のテキスト**: VLMとLLMの違いや、それぞれのプロセスについて議論されている可能性があります。\n\n- **右側の数式とアルゴリズム**: モデルの数学的な部分やプロセスの詳細が書かれている可能性があります。\n\n補足情報に基づき、VLMとLLMの利点や課題を比較して、具体的な使用ケースを検討していることが考えられます。LlamaやLlavaの採用を検討する際に、これらの技術の特性を評価している見込みです。"
    },
    "IMG_7143.jpg": {
        "ocr_text": "11\nScore\n(SCORE)\n(nyale(XXModel(PretrainedMode())\n000.4\nLLM\nselfinit()pdf.11m\ncap\nSelf.emb\njusti\n«Loba\nDseltscure-\nforward()\nWE\nvit\nx:11m(imputo-1m)\n16Jabin\n11.286...\nIbin\n..\nhhpara\n-(.)\n11\n1009\nbf16\nf32\n=\nfor\n(6)\nx\nLCM\nbf16+\n-1.67\n.--\nbr\nLLAを行いしょう",
        "context": "これは私のモデルの実装を議論したときのホワイトボードの写真です．LLMとWE(word embedding)が書いてあります，f32やbf16やf16の精度の話もしてある",
        "chatgpt_summary": "ホワイトボードには、モデルの実装に関する議論が纏められているようです。以下に、提供された補足情報に基づいて内容を説明します。\n\n1. **モデル構造**:\n   - 「LLM」と「WE」（word embedding）が大きく書かれており、これは大規模言語モデルの構造を指しています。\n   - LLMは、おそらくモデルの中核部分で、WEは単語の埋め込み表現に関する部分と考えられます。\n\n2. **精度の議論**:\n   - f32, bf16, f16の記載があり、これは異なる数値表現形式を指しています。\n   - f32（32ビット浮動小数点）、bf16（16ビット半精度浮動小数点、bfloat16とも呼ばれる）、f16（16ビット浮動小数点）それぞれについての議論があります。\n   - 精度やbit数（1ビットや8ビットなど）についても言及されており、モデルのパフォーマンス・精度に関連するテーマを扱っています。\n\n3. **スコアリングと評価（Score）**:\n   - スコアの計算や評価に関する議論も見られます。精度（precision）に関する図表があり、具体的な数値（1.288, 1.678など）も記されています。\n   - 評価指標をどのように改善するかについての言及もあります。\n\n4. **実装に関連する詳細**:\n   - 記述から、「self-attention」や「ML」などの用語が見受けられることから、特に注意メカニズムや機械学習全般についての議論が含まれていると推測できます。\n   - 「KV-cache」や「liena」などの用語も見られ、内部処理やキャッシュメカニズムに関する詳細も含まれているようです。\n\nこの議論は、モデルの精度や実装に関する具体的なポイントをどう最適化し、改善するかに重点を置いているように見受けられます。"
    },
    "IMG_7202.jpg": {
        "ocr_text": "1\n(I(,TX11\n10%10~\nfloor\nBatch=\n->\nKambara\n12/19\nDENEB:\nON-RODUStAuton\nEvaluationMetricforImageCaptioning\nDENEB:AHallucination-RobustAutomaticEvalua\nNO.\nMetricforImageCaptioningAcceptedatACCV20241\nKazukiMatsuda,YuigaWada,KomeiSugiura(KeioUniversity)\nKazukiMatsuda\nKomeiSugiuraLaboratory",
        "context": "ポスターが２種類貼ってある．Long capについて１段追加して指摘を行った．スライド３枚分くらいの分量で",
        "chatgpt_summary": "ホワイトボードには、以下の内容が含まれています。\n\n### ポスター\nホワイトボードには、「Evaluation Metric for Image Captioning」に関する2種類のポスターが貼られています。これらは画像キャプション生成における評価方法に関する詳細な情報を提供しており、それぞれ異なるデータや結果の表を含んでいます。\n\n### 数式の説明\nボードには数式がいくつか記されています。特に、\\( I_{x_i}, T_x \\), \\( Fx \\) などの変数を使った計算の流れが示されています。数式はデータのバッチ処理やパス選択を説明しているようで、図示された矢印が計算の流れを示唆しています。\n\n### 作業メモ\n右下には、今後の作業内容や改善点が記載されています。\n- **ヘッド評価の改善（improve hd. evaluation）**: 評価手法の改善が求められていることを示しています。\n- **Long capに1段追加**: これは長いキャプションに関する追加指摘です。\n- **スライド3枚分の分量**: 作業やプレゼン資料作成の目標を示しています。\n\n### 資料分析\n- 「4696 images -> 2 images -> ex 1 -> sth F1, F2.」という記述があり、画像データの処理過程や評価基準の設定が行われています。\n  \n### その他のメモ\n右側には日付を伴ったリストがあり、個人またはチームの締切や予定を示しているようです。\n\n以上の情報を参考に、画像キャプション生成のプロジェクトで、計算手法と評価基準の改善に取り組んでいることが伺えます。"
    },
    "IMG_7203.jpg": {
        "ocr_text": "bets\nself.forward(mide,candy,hugs)->pbx3\nVI\n10/10~floor\ntrainer,loss(mode,scores,gtseams)\n->\n\"score:\"\nKambara\n12/19\n*ifmode=>for:\n-list[str]\nBatch=",
        "context": "これはモデル構造を相談したときの画像，desc, rel, fluの３観点でMLPを用いて評価値を出力しよという議論をした，その前にLLのforward関数を用たモデル構しようと考えていました",
        "chatgpt_summary": "このホワイトボードには、モデル構造についての議論が記されています。以下は取り上げられているポイントと関連する図表の説明です。\n\n1. **モデル構造の基本アイディア**:\n   - 「LLのforward関数」を基にしたモデル構築の提案があります。これは後にMLP（マルチレイヤパーセプトロン）が使われることに関する議論の前段階とされています。\n  \n2. **評価指標**:\n   - 3つの評価指標: `desc`, `rel`, `flu`があり、これらに対してMLPを用いて評価値を出力するアイディアが示されています。\n\n3. **図表**:\n   - ホワイトボードの左側には、`q-proj`や`LLM_D`といった用語が見られ、モデル設計のディスカッションがされています。\n   - 「self.forward」や「trainer」などの関数呼び出しのロジックが矢印と共に記述されています。\n\n4. **データフロー**:\n   - バッチ処理やテンソルの操作についての記述もあり、それぞれのステップが矢印でつながれています。\n   - 各MLPの出力をまとめ、さらに別の層に流すフローが示されています。\n\n5. **その他**:\n   - ホワイトボードの右には別の情報（1% floorなどのテキスト）が記載されていますが、直接的なモデル議論とは無関係のようです。\n\nこのホワイトボード全体は、機械学習モデルの設計および評価指標に焦点を当てたブレインストーミングセッションを記録したものです。MLPを用いた評価の導入が特に強調されていることがわかります。"
    },
    "IMG_7204.jpg": {
        "ocr_text": "refs\nself.forward(mode,cands,hogy)\nABX3\nDBx3\n-7\ntrainer,loss(mode,scores,grscores\n-R3\n(I(,TX)!\n1%10-",
        "context": "これは私の研究で用いるモデル構造の議論の内容です．具体的にはdesc, rel, fluの観点をmodeとしてMLPに入力して出力することや，LLMを用いたモデル構造にすることなどが書かれている．またMLPやLLMの出力のデータ構造についても説明している",
        "chatgpt_summary": "このホワイトボードには、研究に用いるモデル構造が議論されています。\n\n1. **モデルアーキテクチャ**:\n   - desc, rel, fluといった異なる観点をmodeとして使用し、MLPに入力する構造が示されています。\n   - LLMを用いたモデル構造も言及されています。\n\n2. **データフロー**:\n   - 各モードのデータがMLPの入力として結合され、出力としてモデルが生成するデータの構造が説明されています。図には入力から出力へのフローが矢印で示されています。\n\n3. **コードフラグメント**:\n   - `self.forward`, `trainer.loss`などの関数関連のコード部分が描かれており、このモデルの実装の一部が示唆されています。\n\n4. **図とチャート**:\n   - 異なるモード（例: `list(cnd)`, `scores`）の入力データがどのように処理されるかを示す図が描かれています。\n   - 入力から出力までのデータ処理手順が矢印で視覚的に示されています。\n\n5. **メモ**:\n   - \"q-proj\"や\"nn.Linear (D, 3) Loss\"といったメモがあり、特定の計算や設定が特に強調されています。\n\n6. **特筆点**:\n   - \"UER\", \"WER\"などの指標が図の横に記載されており、これらがモデルの評価に関連する可能性があります。\n\n全体として、ホワイトボードはモデルの構造、データフロー、関数仕様、評価指標について詳述しています。"
    },
    "IMG_7213.jpg": {
        "ocr_text": "4\nしいて手法における新規性とはて\n1.足す?→→どこに?。\n·モデルフ\n井井\n9\n.It?(prompt)\n(2000+KK\nheads",
        "context": "これはデータセットの分割方法や今後の方向性について議論したときのホワイトボード",
        "chatgpt_summary": "# ホワイトボードの内容説明\n\n## 1. 全体の概要\nこのホワイトボードには、**データセットの分割方法**や**評価手法**、**今後の研究の方向性**についての議論が書かれています。LLM（大規模言語モデル）を利用した評価フレームワークを検討している様子が伺えます。\n\n- **データセットの分割**  \n  データの「train」「val」「test」や「dci-train」「dci-test」などの分割があり、それぞれのサンプル数が記載されています。\n  \n- **LoRA の活用**  \n  LLM に対する LoRA（Low-Rank Adaptation）の適用が図示されており、LLM の評価手法についての議論がされています。\n\n- **LLM の評価指標としての利用**  \n  「LLM as a judge（LLM を評価者とする）」という記述があり、LLM を用いた評価の妥当性や強さを議論しているようです。\n\n---\n\n## 2. データセットの分割\nホワイトボードの右下に、データセットの分割についての詳細が書かれています。\n\n| データセット | サンプル数 |\n|-------------|----------|\n| train      | 4000    |\n| val        | 350     |\n| test       | 800     |\n| dci-train  | 100     |\n| dci-test   | 100     |\n\n「x 3 seeds」という記述があり、複数のランダムシードを用いた実験を行っている可能性が高いです。\n\n---\n\n## 3. LLM の評価に関する議論\nホワイトボードの左下には、「LLM as a judge」を用いた評価方法についての議論が見られます。"
    },
    "IMG_7508.jpg": {
        "ocr_text": "GT\n(i)\ndes:4,xix\n3\nFlu:3,2.3\nPolos\n80\ni=1-N\nPolo\n思って了.",
        "context": "これは人間によどのような計算を行うかという議論を行ったときのものです．w/GTではLancersで集めたGTを用いて人間による性能を測るものです．一方w/o GTでは新たに収集するデータから人間による性能を計測するというもの",
        "chatgpt_summary": "このホワイトボードでは、人間による評価の方法に関する議論が行われています。議論は「w/ GT」と「w/o GT」の二つのアプローチに分かれています。\n\n### 1. w/ GT (with Ground Truth)\n- **Lancers:** Lancersを使用して収集したGT（グラウンド・トゥルース）を用いて、どのように人間の性能を評価するかが示されています。\n- **計算方法:** \n  - 各項目A、B、Cに対してGTに対する評価をとり、その平均を計算します。具体的には、`mean(tau(A, GT), tau(B, GT), tau(C, GT))` で評価します。\n\n### 2. w/o GT (without Ground Truth)\n- **新規データ:** 新たに収集するデータに基づいて人間の性能を計測します。\n- **計算方法:**\n  - GTがない場合、データ間の関連性のみを基に評価を行います。`mean(tau(A, mean(B, C)), tau(B, mean(A, C)), tau(C, ...))` のような形で評価します。\n\n### 図表の説明\n- **矢印図:** A、B、Cの間の相互関係を示す図が描かれています。これにより、各データの相互評価の構造を示していると考えられます。\n- **メトリクスと人間の比較:** GTに対するメトリクスおよび人間の評価結果が列挙されています (`metric A`, `human A`等)。\n\nこの議論は、GTを使用した評価方法と、新規データを用いた別のアプローチを比較するものであり、それぞれの方法でどのように性能を測るかが検討されています。"
    },
    "IMG_7510.jpg": {
        "ocr_text": "Exp\nHD\nULM\nWeauthorizemetrics\n-Normal\nLCM\nlongcap.の評価は重要.",
        "context": "これは論文についての議論です．long captionの評価だという内容や，人間がアノテーションしたlong referenceで尺度をauthorizeしようという内容が書かれています",
        "chatgpt_summary": "ホワイトボードの内容は、論文における「long caption」や「long reference」の評価についての議論がなされています。\n\n1. **We authorize metrics**: 評価指標の基準を設けることを示しています。\n\n2. **long cap.の評価は重要**: 長いキャプション（説明文）の評価が重要であることを示しています。\n\n3. **BLEUとかは全くダメ**: 通常のBLEUスコアが、言語モデル（LLM）の評価に適していないことが示されています。\n\n4. **long ref.もベンチマークで尺度をauthorizeしましょう**: 長いリファレンスもベンチマークで基準化すべきであることを示しています。\n\n5. **GPT-4xはsuperhuman**: GPT-4xが非常に高性能であることを指摘しています。\n\n6. **探索 > GPT-4x -> 標をほうふかせない**: 探索がGPT-4xを超える可能性があり、現状では標準を満たすことができないと考えています。\n\nこれらの内容から、論文では長い説明文の評価方法や、その基準設定の重要性について議論が行われていることがわかります。"
    },
    "IMG_7781.jpg": {
        "ocr_text": "6\na\nParaeval」と同じであれば今回のタスクは不要\n5.\nい違う.\n「なぜんが付与したrefを使うべきなのか?\n0\n1\nGPT生成文をrefにして、人間のjudgmentを与える方式ではなぜダメ?」",
        "context": "分野における本論文の立ち位置を議論したもの，Paraevalという既存のデータセットとの区別化をどう行うかと議題",
        "chatgpt_summary": "ホワイトボードの内容を以下に説明します。\n\n1. **Paraevalとの区別**: \n   「Paraeval」との区別が重要であり、同じであれば今回のタスクは不要と考えられています。\n\n2. **人が付与した参照（ref）について**:\n   - 人が付与した参照を使う理由が問われています。具体的には、「なぜ人が付与したrefを使うべきなのか？」という問いです。\n   - また、「GPT生成文をrefにして、人間の judgment を与える方がではなぜダメか」という疑問も提起されています。\n\n3. **authorized vs unauthorized**:\n   - 人による判断として「authorized」と「unauthorized」の比較が行われています。これはおそらくデータの正当性や信頼性に関する議論です。\n\n4. **最終的なまとめ**:\n   - 最終的に、この議論をもとにスピーチをまとめあげ、contribution（貢献）とすることが目標として示されています。\n\nこのホワイトボードは、Paraevalというデータセットとの違いや、データ参照の妥当性に関する議論を記録しています。"
    }
}
